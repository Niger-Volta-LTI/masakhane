# Masakhane Meeting Notes: 09/04/2020

- Time: 19:00
- Number of Attendees: 12
- Notes taken by Jade Abbott

## Data

- Allahsera - Collecting Bamara. Partnered with a centre in CIL in Mali and they have some data. Has the data and is formating the data. 
- Jamiil - virtual meeting to get data. They promise to give you a CD through mail. 
- Blessing - Didn't get a parallel corpus. Only monolingual. 10K sentences. Perhaps it can still be useful. 

## Project and Paper Updates


- Bona & Chris - camera-ready version submitted and have done video video. Fix and increase the size of the dataset. Transformers. 
- Kelechi 
    - Oreva is working on something with Sarah Hooker - on LRL but around model pruning. 
    - Kelechi is working with T5 - new pretraining transformer for English - Nigerian Pidgin translation. 

## Workshop Updates

- WiNLP - 24th April extension
- EMNLP - 1st of June. 

## Masakhane Goals for EMNLP Paper

### Evaluation
- Graham Neubig has agreed to come do a seminar for us on an error analysis technique.
- Julia agrees evaluation aspect is very interesting but also notes how difficult human evaluation is
- She also thinks we should look at some automatic evaluation techniques, while continuing with human evaluation. 
- She recommended we look at the winners of the WMT metric challenge and see if they are better with LRL. 
- A friend built a tool with JoeyNMT for post-editing and markings of errors. open-source. She will share the resources. 

### Transfer Learning Ideas

#### Naive
- Combine all the datasets and how it works. But usually good when training for same target language (opposite of us)

#### Ensembles of Sequence-to-sequence models.
- Julia has some slides of people trying this (but not with multilingual models - which she thinks could work quite well)
- Basically train ensembles of MT models. 

## Open-floor

- Chris:
    - He wants to Check out metrics. Julia to post WMT challenge
    - Interested in Unsupervised Learning Methods like Reinforcement learning methods in NMT. 
    - Julia gave a description about how it's only useful for more niche tasks where search space has already been limited. i.e. domain adaptation
    - She comments on self-training where oddly, the models adapts to the domain if you let it train on it's own outputs (which might ean reinforcement learning is not actually the reason the models are improving, but rather the self-training)  
- Gabriel asks if python is best to learn to do ML - Jade says yes, it's a great place to start